{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d860e14",
   "metadata": {},
   "source": [
    "## Lets learn a few terminologies we will be using in this notebook:\n",
    "- Corpus: Paragraphs or text body or raw word dataset if you are looking for a machine learning analogy\n",
    "- Documents: Sentences\n",
    "- Vocubalary: Set of unique words\n",
    "\n",
    "## What is Text Normalization\n",
    "### Note: The following definitions are specific to ENGLISH Language. They may wary for other language based on their styles. \n",
    "It is a process of converting raw text data into a standard form. Following are some ways to normalize:\n",
    "- Tokenization: Seperation of words based of spaces.\n",
    "- Lemmetization: A method of converting the word to its root word called lemma. For example, sing is the lemma for sang, sung and sings.\n",
    "- Stemming: It is the simpler version of Lemmetization where the suffix of a word is omitted. For example, going stemmed to go removing 'ing'\n",
    "\n",
    "Text normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like periods or exclamation points. Note that tokenization is a type of sentence segmentation but not vice-versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b291a09",
   "metadata": {},
   "source": [
    "We will be using the NLTK library for this notebook so lets install it!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57f4da2d",
   "metadata": {},
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf1da0",
   "metadata": {},
   "source": [
    "## Text Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509bfb66",
   "metadata": {},
   "source": [
    "#### Type 1] Corpus to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5cb8f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2135698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence segmentation is the process of determining the longer processing units consisting of one or more words. This task involves identifying sentence boundaries between words in different sentences. Since most written languages have punctuation marks which occur at sentence boundaries, sentence segmentation is frequently referred to as sentence boundary detection, sentence boundary disambiguation, or sentence boundary recognition. All these terms refer to the same task: determining how a text should be divided into sentences for further processing.\n"
     ]
    }
   ],
   "source": [
    "corpus = \"Sentence segmentation is the process of determining the longer processing units consisting of one or more words. This task involves identifying sentence boundaries between words in different sentences. Since most written languages have punctuation marks which occur at sentence boundaries, sentence segmentation is frequently referred to as sentence boundary detection, sentence boundary disambiguation, or sentence boundary recognition. All these terms refer to the same task: determining how a text should be divided into sentences for further processing.\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fce13ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1]Sentence segmentation is the process of determining the longer processing units consisting of one or more words.\n",
      "2]This task involves identifying sentence boundaries between words in different sentences.\n",
      "3]Since most written languages have punctuation marks which occur at sentence boundaries, sentence segmentation is frequently referred to as sentence boundary detection, sentence boundary disambiguation, or sentence boundary recognition.\n",
      "4]All these terms refer to the same task: determining how a text should be divided into sentences for further processing.\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for document in sent_tokenize(corpus):\n",
    "    print(f'{i}]{document}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce2187d",
   "metadata": {},
   "source": [
    "#### Type 2] Paragraph to word \n",
    "Observe that even symbols are treated as words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98862f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9fab3aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentence', 'segmentation', 'is', 'the', 'process', 'of', 'determining', 'the', 'longer', 'processing', 'units', 'consisting', 'of', 'one', 'or', 'more', 'words', '.', 'This', 'task', 'involves', 'identifying', 'sentence', 'boundaries', 'between', 'words', 'in', 'different', 'sentences', '.', 'Since', 'most', 'written', 'languages', 'have', 'punctuation', 'marks', 'which', 'occur', 'at', 'sentence', 'boundaries', ',', 'sentence', 'segmentation', 'is', 'frequently', 'referred', 'to', 'as', 'sentence', 'boundary', 'detection', ',', 'sentence', 'boundary', 'disambiguation', ',', 'or', 'sentence', 'boundary', 'recognition', '.', 'All', 'these', 'terms', 'refer', 'to', 'the', 'same', 'task', ':', 'determining', 'how', 'a', 'text', 'should', 'be', 'divided', 'into', 'sentences', 'for', 'further', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(corpus)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b400afa",
   "metadata": {},
   "source": [
    "#### Type 3] Sentences to word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9675d458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentence', 'segmentation', 'is', 'the', 'process', 'of', 'determining', 'the', 'longer', 'processing', 'units', 'consisting', 'of', 'one', 'or', 'more', 'words', '.']\n",
      "['This', 'task', 'involves', 'identifying', 'sentence', 'boundaries', 'between', 'words', 'in', 'different', 'sentences', '.']\n",
      "['Since', 'most', 'written', 'languages', 'have', 'punctuation', 'marks', 'which', 'occur', 'at', 'sentence', 'boundaries', ',', 'sentence', 'segmentation', 'is', 'frequently', 'referred', 'to', 'as', 'sentence', 'boundary', 'detection', ',', 'sentence', 'boundary', 'disambiguation', ',', 'or', 'sentence', 'boundary', 'recognition', '.']\n",
      "['All', 'these', 'terms', 'refer', 'to', 'the', 'same', 'task', ':', 'determining', 'how', 'a', 'text', 'should', 'be', 'divided', 'into', 'sentences', 'for', 'further', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "for document in sent_tokenize(corpus):\n",
    "    print(word_tokenize(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6cb2f6",
   "metadata": {},
   "source": [
    "#### Type 4] Regex Tokenizer\n",
    "Build a basic understanding of regular expression from this url: https://www.w3schools.com/python/python_regex.asp. <br>These expression will be useful to solve complex tokenization problems of deteting words like U.S.A, New York,  as one token or even numerical value like decimals, currency etc.\n",
    "<br> Refer for more information: https://www.nltk.org/api/nltk.tokenize.regexp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16a25549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e465bcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New',\n",
       " 'York',\n",
       " 'is',\n",
       " 'located',\n",
       " 'in',\n",
       " 'the',\n",
       " 'U.S.A.',\n",
       " 'that',\n",
       " 'sells',\n",
       " 'the',\n",
       " 'best',\n",
       " 'biscuits',\n",
       " 'for',\n",
       " '$1.28']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The regex patter has been taken from the Stanford professors' Speech and Language Processing book\n",
    "sentence = \"New York is located in the U.S.A. that sells the best biscuits for $1.28\"\n",
    "regex_tokenizer = RegexpTokenizer(r'''(?x)          # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    ''')\n",
    "regex_tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a635e",
   "metadata": {},
   "source": [
    "### Some Advanced Toeknization topics to be covered as I build my understanding of NLP\n",
    "<br> There are roughly two classes of tokenization algorithms: <br>**Top-down algorithm (rule-based) and Bottom-Up Algorithm (Byte-Pair encoding)**\n",
    "<br> In Top down algorithm we decide how to seperate the token based on symbols, regular expressions or on words itself. However, in the Byte Pair encoding, tokens are decided automatically based on the training data set. This model is heavily used in LLMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74548a8",
   "metadata": {},
   "source": [
    "## Text Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "846abec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a48fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\", \"jumps\", \"goes\", \"running\", \"fairly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b90af8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eat', 'jump', 'goe', 'run', 'fairli']\n"
     ]
    }
   ],
   "source": [
    " # Apply stemming to each word\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be83643",
   "metadata": {},
   "source": [
    "As you can see from the output above the word happily has not been stemmed properly (cahnge in form of the word, change in meaning of the word, etc) which is a major drawback of this technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d14d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a2a2af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_stemmer = RegexpStemmer('ing$|s$|e$|able$')\n",
    "regex_stemmer.stem('works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "815ea9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cde85b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eat', 'jump', 'goe', 'run', 'fair']\n"
     ]
    }
   ],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')\n",
    "stemmed_words = [snowball_stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05847b3d",
   "metadata": {},
   "source": [
    "Observe that even though Snowball performs better than PotterStemmer, it still does not work properly with all the words. Hence these techniques are useful only for simpler languge processing. To get rid of all these disadvantages we use Lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91324b80",
   "metadata": {},
   "source": [
    "## Text Lemmatization\n",
    "<br> Lemmatization is an advanced version of Stemming which find the root word called *\"Lemma\"* of the givem words.\n",
    "<br> We will use a few basic Lemmatizers in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581467c",
   "metadata": {},
   "source": [
    "### WordNetLemmatizer\n",
    "<br> It is built on the morphy function of the wordnet library. It utilizes the WordNetCorpus class to search for the lemma. The function, in simple terms, checks for the base foem of the input word in an intelligent and thorough manner.\n",
    "<br> reference link for more on morphy: https://wordnet.princeton.edu/documentation/morphy7wn#:~:text=A%20set%20of%20morphology%20functions,found%20in%20the%20WordNet%20database.\n",
    "<br> Two important parameters should be used: the word and The Part Of Speech tag. Valid options are “n” for nouns, “v” for verbs, “a” for adjectives, “r” for adverbs and “s” for satellite adjectives. By default, the POS is Noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1ef04d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c70ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "87006585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"going\") # since the POS is by default set as 'n', the output is unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a66659c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"going\",pos='v') #Since the word going is verb, it now gave the correct output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c727ce00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat', 'jump', 'go', 'run', 'fairly']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"eating\", \"jumps\", \"goes\", \"running\", \"fairly\"]\n",
    "[lemmatizer.lemmatize(w,pos='v') for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946535a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
